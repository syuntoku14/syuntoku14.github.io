[{"authors":null,"categories":null,"content":"I am a Ph.D. candidate at The University of Tokyo in Japan (2022-). My research interests include reinforcement learning theory, bandit algorithms, deep learning, and robotics.\n  Download my resumé.\n","date":1645747200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1645747200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. candidate at The University of Tokyo in Japan (2022-). My research interests include reinforcement learning theory, bandit algorithms, deep learning, and robotics.\n  Download my resumé.","tags":null,"title":"Toshinori Kitamura","type":"authors"},{"authors":["Toshinori Kitamura"],"categories":["Internship"],"content":"2021年6月から2022年2月まで, オムロンサイニックエックス (OSX) にて研究インターンをさせていただきました. これはOSXインターンについてまとめたポストになります. 元インターンの奥村さんのポストも参考になるのでぜひ.\n自己紹介  現在は奈良先端科学技術大学院大学の修士２年, 2022年の4月から東京大学で博士課程に進学予定の学生です. 学部の頃はロボットよりの研究をしていましたが, 修士課程では強化学習の理論やアルゴリズムがメインの研究をやっていました. 詳しくはホームページを見てね.  インターンまでの経緯 M2の前半で主著を1つ \u0026amp; 共著を2つ生やすことができたので, M2の後半はインターンやりたいなあ〜って思っていたところ, 研究室経由でメンターの米谷さんからOSXインターンのお誘いをいただきました. 待遇や時期的にもかなり好条件だったので即返事をし, フルタイムで半年以上の研究インターンに参加させていただきました. インターンへの応募はここに詳細があります. 興味がある人はぜひ.\n何をやったのか    強化学習の理論と応用の実験をサポートしたライブラリ ShinRL を開発しました. 当初は米谷さんの提案で別の強化学習のテーマに取り組んでおり, 僕が個人的に開発していたShinRLを使ってデバッグ \u0026amp; アルゴリズムの実装を進めていました. もっとShinRLを高速化したい, 実装をキレイにしたい, などの理由で途中からShinRLの開発にシフトし始め, 最終的にはOSXとしてオフィシャルに公開するのを目標にテーマを変更しました (米谷さんには快く承諾していただきました. 圧倒的感謝です).\n最初はPyTorchで実装していたShinRLですが, 速度面や実装の単純さ, 勉強目的からJAXを利用して実装することに. 米谷さんもJAXは初めてだったので, お互いに情報を共有しあって実装のアイデアを提案しまくりました. インターンは完全リモートでしたが,\n GitLabのissueで毎日進捗管理 slackで必要に応じて質問を投げ合う 週一のミーティング  を徹底して頂き, 遠隔でも充実した開発環境が整っていたと思います. 週一のミーティングでなんとなく出したアイディアが実装に活かされることがよくあったので, やっぱ定期的な同期コミュニケーションは大事ですね (例えばMixInシステムやJAX自体もミーティングでなんとなく出したアイディアになります. 論文参照).\nライブラリ開発の流れ もともとあったライブラリのコンセプトをベースに,\n 実装の柔軟さ 単純さ 速さ  を向上させようとライブラリの破壊と創造を繰り返しました. とにかく 「僕がアイディアを実装して米谷さんに説明」 → 「伝わらなければ作り直し」 のサイクルを回しまくり, ほぼ常にコーディングをしていた感じです. ある程度形になってからはNeuripsのDeep RL Workshopで発表するために論文化の作業に移り, ShinRLのユースケースを書く作業も並行して進めていました. 米谷さんによる論文執筆の補助のおかげで, なんとか論文も発表できるレベルまでもっていけました (圧倒的感謝２です).\nよかったこと JAXや論文執筆力が身についたのはもちろんなんですが, わかりやすくコードを書く力 が改めて成長しました. 以前から一応リーダブルコードに書いてあることを意識してコーディングはしていたのですが, 自分一人で開発していると限界があります. OSXインターンでは週一回のミーティングで進捗やコードを説明する機会があり, ミーティングで伝わらない話は基本的にShinRLのユーザーにも伝わらないと思っていました. 自分で「あれ, ここは分かりづらいな？」と思った箇所は米谷さんに説明してみて, 反応を見て分かりづらそうだったら書き直し, を繰り返すうちにコードの可読性を上げる能力が向上していきました. 個人的に研究室や以前のインターンなどでも週一くらいでミーティングはあったんですが, コードレビューに近いことはしたことがなかったので今回はめちゃくちゃ良かったですね. 米谷さんも頻繁にコードの改良に参加してくれるので, 自分では思いつかない実装アイディアもたくさん出ました.\n反省点    もっとテスト駆動開発を徹底したほうが良かったかもしれません. 開発は基本的に\n GitLabのissueやslackで「〜を実装しようかなって考えてます」って報告 米谷さんの意見を踏まえて実装 実装した後でissueやミーティングで説明 反応を見て修正  のサイクルでした. 1と2では文章だけで説明していたので, 説明のし忘れや勘違いなどが頻発しました. 僕自身も後で実装したのが「なんか違うな…」になることが多く, サイクルを回す回数が無駄に多くなってしまいました. 反省点として, 1の時点で通っておくべきテストコードをたくさん書けばよかったなと思っています (研究してると忘れがちになっちゃう…). 一応pytestによる動作確認はなるべく徹底していたんですが, それがミーティングや要件定義に活かされていませんでした (次の開発では徹底していきたいです. 反省.).\nまとめ OSXのインターンは全体としてとても良かったです. サポートがとにかく手厚いのでリモートでもほぼ問題ありませんでした. 研究に少し余裕が出てきたり, 新しいことに挑戦したい学生の方には参加してみて欲しいと思います. 余談ですが, インターン期間中に２回だけオフィスの方に顔を出したことがあります. キレイなオフィス \u0026amp; 立地的に便利なところ だったので現地で働くともっと楽しいかも？ OSXインターンめっちゃ良いので, 気になる人はぜひ応募してみてね！\n","date":1645747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645747200,"objectID":"d53228f62706d177c0e154f5efc0eea1","permalink":"https://syuntoku14.github.io/post/omron/","publishdate":"2022-02-25T00:00:00Z","relpermalink":"/post/omron/","section":"post","summary":"2021年6月から2022年2月まで, [オムロンサイニックエックス (OSX)](https://www.omron.com/sinicx/) にて研究インターンをさせていただきました. これはOSXインターンについてまとめたポストになります.","tags":["Research","Internship"],"title":"OMRON SINIC X (OSX) のインターン感想","type":"post"},{"authors":["Lingwei Zhu","Toshinori Kitamura","Matsubara Takamitsu"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709127,"objectID":"a6700f544ef524edf272568133eed7c9","permalink":"https://syuntoku14.github.io/publication/zhu-2021-cautious-ac/","publishdate":"2022-02-24T13:25:39.083232Z","relpermalink":"/publication/zhu-2021-cautious-ac/","section":"publication","summary":"","tags":[],"title":"Cautious Actor-Critic","type":"publication"},{"authors":["Lingwei Zhu","Toshinori Kitamura","Takamitsu Matsubara"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709304,"objectID":"ba63c75c3c8e4a836f02eb540c1e0855","permalink":"https://syuntoku14.github.io/publication/zhu-2021-cautious/","publishdate":"2022-02-24T13:28:26.469581Z","relpermalink":"/publication/zhu-2021-cautious/","section":"publication","summary":"","tags":[],"title":"Cautious Policy Programming: Exploiting KL Regularization in Monotonic Policy Improvement for Reinforcement Learning","type":"publication"},{"authors":["Toshinori Kitamura","Lingwei Zhu","Takamitsu Matsubara"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709011,"objectID":"e5d8d35fc12c48660843119501bcca02","permalink":"https://syuntoku14.github.io/publication/kitamura-2021-geometric/","publishdate":"2022-02-24T13:23:31.052161Z","relpermalink":"/publication/kitamura-2021-geometric/","section":"publication","summary":"","tags":[],"title":"Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning","type":"publication"},{"authors":["Toshinori Kitamura","Ryo Yonetani"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709207,"objectID":"8f6d944d2cbecc87985ed40ed0134457","permalink":"https://syuntoku14.github.io/publication/kitamura-2021-shinrl/","publishdate":"2022-02-24T13:26:47.805697Z","relpermalink":"/publication/kitamura-2021-shinrl/","section":"publication","summary":"","tags":[],"title":"ShinRL: A Library for Evaluating RL Algorithms from Theoretical and Practical Perspectives","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://syuntoku14.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c8d738e9720316240100a44b7bb7527d","permalink":"https://syuntoku14.github.io/project/fusion2urdf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/fusion2urdf/","section":"project","summary":"A fusion360 add-in which converts fusion360 model to urdf(Universal Robotic Description Format) file.","tags":["Other"],"title":"fusion2urdf","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9df5c858555e8cbf5fd3f5bd99bec599","permalink":"https://syuntoku14.github.io/project/rlil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/rlil/","section":"project","summary":"A PyTorch Library for Building Reinforcement Learning and Imitation Learning Agents","tags":["Reinforcement Learning"],"title":"PyTorch-RL-IL (rlil)","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0c8a6a37ff445156911643b3f7025748","permalink":"https://syuntoku14.github.io/project/shinrl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/shinrl/","section":"project","summary":"A Library for Evaluating RL Algorithms from Theoretical and Practical Perspectives.","tags":["Reinforcement Learning"],"title":"ShinRL","type":"project"}]