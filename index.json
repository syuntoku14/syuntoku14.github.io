[{"authors":null,"categories":null,"content":"I am a Ph.D. student at the University of Tokyo advised by Dr. Tadashi Kozuno and Prof. Yutaka Matsuo. I received my master’s degree at Nara Institute of Science and Technology advised by Prof. Takamitsu Matsubara.\n  Download my resumé.\n","date":1645747200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1645747200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. student at the University of Tokyo advised by Dr. Tadashi Kozuno and Prof. Yutaka Matsuo. I received my master’s degree at Nara Institute of Science and Technology advised by Prof.","tags":null,"title":"Toshinori Kitamura","type":"authors"},{"authors":["Toshinori Kitamura","Tadashi Kozuno","Masahiro Kato","Yuki Ichihara","Soichiro Nishimori","Akiyoshi Sannai","Sho Sonoda","Wataru Kumagai","Yutaka Matsuo"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707150312,"objectID":"f98569c245a955d302731eb1def1c694","permalink":"https://syuntoku14.github.io/publication/kitamura-2024-policy/","publishdate":"2024-02-05T16:25:08.114154Z","relpermalink":"/publication/kitamura-2024-policy/","section":"publication","summary":"","tags":[],"title":"A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees","type":"publication"},{"authors":null,"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"76092fb2c26664a44abf7dc41fde1d4d","permalink":"https://syuntoku14.github.io/teaching/deeplearning/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/teaching/deeplearning/","section":"teaching","summary":"","tags":null,"title":"Deep Learning Lecture (The University of Tokyo)","type":"teaching"},{"authors":null,"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"e12ae5d6b4549d8ac58bb587b0a0c401","permalink":"https://syuntoku14.github.io/teaching/deep-rl/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/teaching/deep-rl/","section":"teaching","summary":"","tags":null,"title":"Deep RL Spring Seminar (The University of Tokyo)","type":"teaching"},{"authors":[" 小津野将"," 北村俊徳"," 市原有生希"," 萩原誠"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706677707,"objectID":"e6cb6c4c17c24ddb8adb498d4dcdeee2","permalink":"https://syuntoku14.github.io/publication/%E5%B0%8F%E6%B4%A5%E9%87%8E%E5%B0%86-2023-%E6%8B%9B%E5%BE%85%E8%AC%9B%E6%BC%94/","publishdate":"2024-01-31T05:08:22.498539Z","relpermalink":"/publication/%E5%B0%8F%E6%B4%A5%E9%87%8E%E5%B0%86-2023-%E6%8B%9B%E5%BE%85%E8%AC%9B%E6%BC%94/","section":"publication","summary":"","tags":[],"title":"(OS 招待講演) 逐次意思決定における諸問題設定と問題に関する事前知識が性能保証に及ぼす影響について","type":"publication"},{"authors":["Toshinori Kitamura","Tadashi Kozuno","Yunhao Tang","Nino Vieillard","Michal Valko","Wenhao Yang","Jincheng Mei","Pierre Ménard","Mohammad Gheshlaghi Azar","Rémi Munos"," others"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685353218,"objectID":"483dee7e894f3c1d81d45f81882f09e4","permalink":"https://syuntoku14.github.io/publication/kitamura-2023-regularization/","publishdate":"2023-05-29T09:40:18.577881Z","relpermalink":"/publication/kitamura-2023-regularization/","section":"publication","summary":"","tags":[],"title":"Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice","type":"publication"},{"authors":["Toshinori Kitamura"],"categories":["Internship"],"content":"2021年6月から2022年2月まで, オムロンサイニックエックス (OSX) にて研究インターンをさせていただきました. これはOSXインターンについてまとめたポストになります. 元インターンの奥村さんのポストも参考になるのでぜひ.\n自己紹介  現在は奈良先端科学技術大学院大学の修士２年, 2022年の4月から東京大学で博士課程に進学予定の学生です. 学部の頃はロボットよりの研究をしていましたが, 修士課程では強化学習の理論やアルゴリズムがメインの研究をやっていました. 詳しくはホームページを見てね.  インターンまでの経緯 M2の前半で主著を1つ \u0026amp; 共著を2つ生やすことができたので, M2の後半はインターンやりたいなあ〜って思っていたところ, 研究室経由でメンターの米谷さんからOSXインターンのお誘いをいただきました. 待遇や時期的にもかなり好条件だったので即返事をし, フルタイムで半年以上の研究インターンに参加させていただきました. インターンへの応募はここに詳細があります. 興味がある人はぜひ.\n何をやったのか    強化学習の理論と応用の実験をサポートしたライブラリ ShinRL を開発しました. 当初は米谷さんの提案で別の強化学習のテーマに取り組んでおり, 僕が個人的に開発していたShinRLを使ってデバッグ \u0026amp; アルゴリズムの実装を進めていました. もっとShinRLを高速化したい, 実装をキレイにしたい, などの理由で途中からShinRLの開発にシフトし始め, 最終的にはOSXとしてオフィシャルに公開するのを目標にテーマを変更しました (米谷さんには快く承諾していただきました. 圧倒的感謝です).\n最初はPyTorchで実装していたShinRLですが, 速度面や実装の単純さ, 勉強目的からJAXを利用して実装することに. 米谷さんもJAXは初めてだったので, お互いに情報を共有しあって実装のアイデアを提案しまくりました. インターンは完全リモートでしたが,\n GitLabのissueで毎日進捗管理 slackで必要に応じて質問を投げ合う 週一のミーティング  を徹底して頂き, 遠隔でも充実した開発環境が整っていたと思います. 週一のミーティングでなんとなく出したアイディアが実装に活かされることがよくあったので, やっぱ定期的な同期コミュニケーションは大事ですね (例えばMixInシステムやJAX自体もミーティングでなんとなく出したアイディアになります. 論文参照).\nライブラリ開発の流れ もともとあったライブラリのコンセプトをベースに,\n 実装の柔軟さ 単純さ 速さ  を向上させようとライブラリの破壊と創造を繰り返しました. とにかく 「僕がアイディアを実装して米谷さんに説明」 → 「伝わらなければ作り直し」 のサイクルを回しまくり, ほぼ常にコーディングをしていた感じです. ある程度形になってからはNeuripsのDeep RL Workshopで発表するために論文化の作業に移り, ShinRLのユースケースを書く作業も並行して進めていました. 米谷さんによる論文執筆の補助のおかげで, なんとか論文も発表できるレベルまでもっていけました (圧倒的感謝２です).\nよかったこと JAXや論文執筆力が身についたのはもちろんなんですが, わかりやすくコードを書く力 が改めて成長しました. 以前から一応リーダブルコードに書いてあることを意識してコーディングはしていたのですが, 自分一人で開発していると限界があります. OSXインターンでは週一回のミーティングで進捗やコードを説明する機会があり, ミーティングで伝わらない話は基本的にShinRLのユーザーにも伝わらないと思っていました. 自分で「あれ, ここは分かりづらいな？」と思った箇所は米谷さんに説明してみて, 反応を見て分かりづらそうだったら書き直し, を繰り返すうちにコードの可読性を上げる能力が向上していきました. 個人的に研究室や以前のインターンなどでも週一くらいでミーティングはあったんですが, コードレビューに近いことはしたことがなかったので今回はめちゃくちゃ良かったですね. 米谷さんも頻繁にコードの改良に参加してくれるので, 自分では思いつかない実装アイディアもたくさん出ました.\n反省点    もっとテスト駆動開発を徹底したほうが良かったかもしれません. 開発は基本的に\n GitLabのissueやslackで「〜を実装しようかなって考えてます」って報告 米谷さんの意見を踏まえて実装 実装した後でissueやミーティングで説明 反応を見て修正  のサイクルでした. 1と2では文章だけで説明していたので, 説明のし忘れや勘違いなどが頻発しました. 僕自身も後で実装したのが「なんか違うな…」になることが多く, サイクルを回す回数が無駄に多くなってしまいました. 反省点として, 1の時点で通っておくべきテストコードをたくさん書けばよかったなと思っています (研究してると忘れがちになっちゃう…). 一応pytestによる動作確認はなるべく徹底していたんですが, それがミーティングや要件定義に活かされていませんでした (次の開発では徹底していきたいです. 反省.).\nまとめ OSXのインターンは全体としてとても良かったです. サポートがとにかく手厚いのでリモートでもほぼ問題ありませんでした. 研究に少し余裕が出てきたり, 新しいことに挑戦したい学生の方には参加してみて欲しいと思います. 余談ですが, インターン期間中に２回だけオフィスの方に顔を出したことがあります. キレイなオフィス \u0026amp; 立地的に便利なところ だったので現地で働くともっと楽しいかも？ OSXインターンめっちゃ良いので, 気になる人はぜひ応募してみてね！\n","date":1645747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645747200,"objectID":"d53228f62706d177c0e154f5efc0eea1","permalink":"https://syuntoku14.github.io/post/omron/","publishdate":"2022-02-25T00:00:00Z","relpermalink":"/post/omron/","section":"post","summary":"2021年6月から2022年2月まで, [オムロンサイニックエックス (OSX)](https://www.omron.com/sinicx/) にて研究インターンをさせていただきました. これはOSXインターンについてまとめたポストになります.","tags":["Research","Internship"],"title":"OMRON SINIC X (OSX) のインターン感想","type":"post"},{"authors":["Tadashi Kozuno","Wenhao Yang","Nino Vieillard","Toshinori Kitamura","Yunhao Tang","Jincheng Mei","Pierre Ménard","Mohammad Gheshlaghi Azar","Michal Valko","Rémi Munos"," others"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671163914,"objectID":"ae13bd71cbd462094f386e5ea7df40c8","permalink":"https://syuntoku14.github.io/publication/kozuno-2022-kl/","publishdate":"2022-12-16T04:11:54.356773Z","relpermalink":"/publication/kozuno-2022-kl/","section":"publication","summary":"","tags":[],"title":"KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal","type":"publication"},{"authors":["Lingwei Zhu","Toshinori Kitamura","Matsubara Takamitsu"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709127,"objectID":"a6700f544ef524edf272568133eed7c9","permalink":"https://syuntoku14.github.io/publication/zhu-2021-cautious-ac/","publishdate":"2022-02-24T13:25:39.083232Z","relpermalink":"/publication/zhu-2021-cautious-ac/","section":"publication","summary":"","tags":[],"title":"Cautious Actor-Critic","type":"publication"},{"authors":["Lingwei Zhu","Toshinori Kitamura","Takamitsu Matsubara"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709304,"objectID":"ba63c75c3c8e4a836f02eb540c1e0855","permalink":"https://syuntoku14.github.io/publication/zhu-2021-cautious/","publishdate":"2022-02-24T13:28:26.469581Z","relpermalink":"/publication/zhu-2021-cautious/","section":"publication","summary":"","tags":[],"title":"Cautious Policy Programming: Exploiting KL Regularization in Monotonic Policy Improvement for Reinforcement Learning","type":"publication"},{"authors":["Toshinori Kitamura","Lingwei Zhu","Takamitsu Matsubara"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709011,"objectID":"e5d8d35fc12c48660843119501bcca02","permalink":"https://syuntoku14.github.io/publication/kitamura-2021-geometric/","publishdate":"2022-02-24T13:23:31.052161Z","relpermalink":"/publication/kitamura-2021-geometric/","section":"publication","summary":"","tags":[],"title":"Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning","type":"publication"},{"authors":["Toshinori Kitamura","Ryo Yonetani"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645709207,"objectID":"8f6d944d2cbecc87985ed40ed0134457","permalink":"https://syuntoku14.github.io/publication/kitamura-2021-shinrl/","publishdate":"2022-02-24T13:26:47.805697Z","relpermalink":"/publication/kitamura-2021-shinrl/","section":"publication","summary":"","tags":[],"title":"ShinRL: A Library for Evaluating RL Algorithms from Theoretical and Practical Perspectives","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://syuntoku14.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a3a3682e38223938dc7be997a770c960","permalink":"https://syuntoku14.github.io/project/uopt-rpgpd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/uopt-rpgpd/","section":"project","summary":"Official Implementations of \"A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees","tags":["Reinforcement Learning"],"title":"A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c8d738e9720316240100a44b7bb7527d","permalink":"https://syuntoku14.github.io/project/fusion2urdf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/fusion2urdf/","section":"project","summary":"A fusion360 add-in which converts fusion360 model to urdf(Universal Robotic Description Format) file.","tags":["Other"],"title":"fusion2urdf","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9df5c858555e8cbf5fd3f5bd99bec599","permalink":"https://syuntoku14.github.io/project/rlil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/rlil/","section":"project","summary":"A PyTorch Library for Building Reinforcement Learning and Imitation Learning Agents","tags":["Reinforcement Learning"],"title":"PyTorch-RL-IL (rlil)","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0c8a6a37ff445156911643b3f7025748","permalink":"https://syuntoku14.github.io/project/shinrl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/shinrl/","section":"project","summary":"A Library for Evaluating RL Algorithms from Theoretical and Practical Perspectives.","tags":["Reinforcement Learning"],"title":"ShinRL","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"507cbf0c2e65d7d17419234b297e3dd2","permalink":"https://syuntoku14.github.io/project/dvw/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/dvw/","section":"project","summary":"Official Implementations of \"Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs\"","tags":["Reinforcement Learning"],"title":"Variance Weighted MDVI","type":"project"},{"authors":["Toshinori Kitamura"],"categories":["annual review"],"content":"今年は主に強化学習における統計的学習理論について勉強してきました． これはその勉強内容とやり方の備忘録です．\n勉強のやり方（Shumi-Noteについて） １月からずっとShumi-Noteを使って勉強内容を記録してきました． １年で約２００記事くらい書いたので，２日に１記事以上のペースで何か書いていることになります． Shumi-Noteのテンプレートはこちら． Shumi-Note自体は下の画像みたいな感じ．\n   今年から始めた習慣ですが，やってみてかなり良かったです． TwitterでGabriel Peyré先生が一日一つCSに関する話とその実装をツイートをしているのを見たのがきっかけです．\n基本方針として，「インプットした内容を，なるべく人に説明するような書き方で，Pythonのコードとともにまとめる」を意識しました． また，次のことにも気をつけていました．\n 書いた内容は日付とともにREADMEにまとめる．  どれくらいの季節に何を集中的に勉強していたか振り返りやすくなり，進捗状況の把握に役立ちました．   一瞬で完了し，再現性の高い実装を心がける．  Shumi-Noteでは強化学習アルゴリズムをたくさん実装しましたが，ほとんどはnumpy (or jax.numpy)で完結しています．深層学習系は実装コストが高いため，今回は見送りました（理論保証がついてる深層学習系の話はそのうちやりたいですね）．   ストレスにならないように自分のペースで書く．  丁寧に書いた記事も，かなり適当に書いた記事もあります．記事は下の画像みたいな感じ．       勉強の内容 強化学習の理論をやるために必要な知識を勉強していました． ここではその中でも体系的な知識をつけるために役に立った資料をまとめておきます．細かい知識の話はアドベントカレンダーにでも書こうと思います． 基本的に勉強した内容はShumi-Noteにまとめ，理解を深めていました．\n測度論的確率論（去年の中旬〜今年の初旬） バンディットや強化学習の理論を理解するために確率論はとても重要です．恥ずかしながら博士課程１年目の半ばごろまで測度論的確率論に触れてこなかったので苦労しました． 用語と定義を覚えないと話にならないので，下の３つの本に出てくる用語や定義，定理の証明を読み込んでいました．（GoodnoteとAnkiを使っていました．）\n 測度・確率・ルベーグ積分 応用への最短コース (KS理工学専門書)  実際この本は応用への最短コースでした．よく使う話が短く，しかし丁寧にまとまっており，非常に助かりました．   舟木確率論  わからないところはこれを適宜参考にしてました．   東大の鈴木先生の講義  東大の鈴木先生の確率数理要論を受講してました．（成績は「可」でした．トホホ…）   Bandit Algorithms  とても良い本です．Part1にバンディットに必要な確率論の話が体系的にまとまっているので，この本をひたすら読む＆章末問題を解きまくっていました．    主なShumi-Noteの記録：このへん\nバンディット（ずっと） バンディットを理解しないと強化学習に置ける探索の話ができません． 以下の資料で体系的に勉強しました．\n Bandit Algorithms  とにかくこれを読み込んでいました． また，友人＆メンターと章末問題を解きまくる会を毎週開催していました．   バンディット問題の理論とアルゴリズム  証明に詰まったときに参考になります．こちらのほうがBandit Algorithmsよりもわかりやすい説明もいくつかありました．   RL Theory and Algorithms  ６章がバンディットの話です．証明が非常にわかりやすいので参考になります．    主なShumi-Noteの記録：このへん\n強化学習（ずっと） 修士までは強化学習 (機械学習プロフェッショナルシリーズ)をひたすら読み込んでいました． 博士からは強化学習についての統計的学習理論を，以下の資料をベースに勉強していました．\n Csaba先生のRLTheory講義  これの翻訳会に参加していました．非常によくまとまった資料です．ICMLに出した論文でも，ここで勉強した内容がとても参考になりました．   RL Theory and Algorithms  メインで勉強していた本です．現在進行系でこれの勉強会を主催しています．担当者が一つの章を読み込んで，それを画面シェアで聴講者に説明する形式で勉強会していました．この資料の証明はかなり読みやすく，論文執筆で重宝しています．   Convex Optimization  Boyd本です．過去に何度か読んでいましたが，改めて読み直していました．強化学習でもめちゃめちゃ使います．この話は別の記事で書きます．    主なShumi-Noteの記録：このへん\n来年頑張りたい話  Bertsekas先生のDynamic Programming and Optimal Control：買ったけど積んでる．読まねば… Concentration inequalities：読まなきゃなあ〜って気持ちのまま一年が過ぎました． 統計的学習理論 (機械学習プロフェッショナルシリーズ)：ちょこちょこ読んでますが，未だ熟読せずに一年が過ぎました． 機械学習のための連続最適化：同上．論文で使うところだけ読んでましたが，未だ熟読せずに一年が過ぎました．  総括 これから強化学習の統計的学習理論を始める方は，とりあえずBandit AlgorithmsとRL Theory and Algorithmsが読めるように勉強し，それぞれの章末についているBibliographyの論文を参照していけば必要な知識はつきそうです． 僕自身まだまだ勉強中なので，一緒に勉強したい方がいればTwitterなどでお気軽に声かけてください．\n今年はのんびり勉強できた一年で，非常に楽しかったです．特にShumi-Noteの実装をしているときはかなり楽しいですね．お気に入りの記事や実装については別のブログにまとめようと思います． 来年も頑張るぞ．\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67f1738c060bf0f8027645078a91fc57","permalink":"https://syuntoku14.github.io/post/rl-theory-2023/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/rl-theory-2023/","section":"post","summary":"博士課程２年 in 2023","tags":["Research","RL"],"title":"強化学習の勉強の備忘録","type":"post"}]